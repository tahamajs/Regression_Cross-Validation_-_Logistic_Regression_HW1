
# **ML Homework 1 â€“ Regression, Cross-Validation & Logistic Regression**

### **University of Tehran â€“ Machine Learning Course**

## **Overview**

This assignment focuses on  **fundamental concepts in machine learning** , including:

* **Cross-Validation**
* **Regularization (L1 & L2)**
* **Linear Regression (Gradient Descent & Normal Equations)**
* **Maximum Likelihood Estimation**
* **Logistic Regression & Classification Metrics**

The goal is to implement and analyze these models using Python.

---

## **Assignment Structure**

### ðŸ“Œ **Question 1: Cross-Validation**

* **Concepts:**
  * Purpose of **Cross-Validation** in machine learning.
  * Types of cross-validation ( **k-Fold, Leave-One-Out, etc.** ).
  * Metric-based evaluation of models.
* **Implementation:**
  * Implement  **k-Fold Cross-Validation** .
  * Compare model performance on different validation folds.

### ðŸ“Œ **Question 2: Regularization in Machine Learning**

* **Concepts:**
  * Difference between **L1 Regularization (Lasso)** and  **L2 Regularization (Ridge)** .
  * Effect of **penalty term (Î»)** on model weights.
* **Implementation:**
  * Implement **L1 & L2 regularization** for regression.
  * Compute **closed-form solution** for Ridge Regression.

### ðŸ“Œ **Question 3: Maximum Likelihood Estimation (MLE)**

* **Concepts:**
  * Deriving the **log-likelihood function** for linear regression.
  * Finding **MLE estimates** by minimizing squared error.
* **Implementation:**
  * Compute  **log-likelihood function** .
  * Optimize parameters using  **MLE** .

### ðŸ“Œ **Question 4: Exponential Regression Model**

* **Concepts:**
  * Regression of the form:
    y=ewxy = e^{wx}
  * Optimization using  **gradient descent** .
* **Implementation:**
  * Compute **loss function** for exponential regression.
  * Derive  **gradient descent update rule** .

### ðŸ“Œ **Question 5: Linear Regression - Gradient Descent vs Normal Equations**

* **Concepts:**
  * Compare **Gradient Descent** vs **Normal Equations** for  **linear regression** .
  * Effect of **feature scaling** on convergence speed.
* **Implementation:**
  * Implement **gradient descent** from scratch.
  * Implement **Normal Equation** solution.
  * Compute **Mean Squared Error (MSE)** for both methods.

### ðŸ“Œ **Question 6: Logistic Regression for Diabetes Classification**

* **Concepts:**
  * Logistic regression for binary classification.
  * Evaluating model using:
    * **Confusion Matrix**
    * **Accuracy**
    * **Precision & Recall**
* **Implementation:**
  * Perform  **Exploratory Data Analysis (EDA)** .
  * Train a  **Logistic Regression classifier** .
  * Compute  **confusion matrix & performance metrics** .

---

## **Project Structure**

```
ML_HW1/
â”‚â”€â”€ data/                     # Dataset files
â”‚â”€â”€ notebooks/                # Jupyter notebooks for implementation
â”‚â”€â”€ src/                      # Python scripts for regression models
â”‚   â”œâ”€â”€ cross_validation.py   # k-Fold Cross-Validation
â”‚   â”œâ”€â”€ regularization.py     # Ridge & Lasso Regression
â”‚   â”œâ”€â”€ mle_regression.py     # Maximum Likelihood Estimation
â”‚   â”œâ”€â”€ exponential_reg.py    # Exponential Regression
â”‚   â”œâ”€â”€ linear_regression.py  # Linear Regression (GD & Normal Eq)
â”‚   â”œâ”€â”€ logistic_regression.py# Logistic Regression
â”‚   â”œâ”€â”€ evaluation.py         # Performance metrics computation
â”‚â”€â”€ results/                  # Plots and evaluation reports
â”‚â”€â”€ README.md                 # This README file
```

---

---

## **Evaluation Metrics**

* **Regression Models:**
  * **Mean Squared Error (MSE)**
  * **Root Mean Squared Error (RMSE)**
* **Classification Models:**
  * **Confusion Matrix**
  * **Accuracy**
  * **Precision & Recall**
  * **F1-Score**

---

## **Results Summary**

* **Cross-Validation improves model generalization** .
* **L1 Regularization (Lasso) leads to sparse features, L2 (Ridge) shrinks coefficients** .
* **MLE and Least Squares give equivalent solutions for linear regression** .
* **Gradient Descent requires tuning of learning rate, Normal Equation is faster for small datasets** .
* **Logistic Regression performs well for diabetes classification, with accuracy analysis** .

---

## **References**

* **Pattern Recognition and Machine Learning â€“ Bishop**
* **The Elements of Statistical Learning â€“ Hastie, Tibshirani, Friedman**
* **Andrew Ngâ€™s Machine Learning Course (Stanford CS229)**
